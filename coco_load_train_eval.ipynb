{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coco_load-train-eval3ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE24pDGf3WDc"
      },
      "source": [
        "# Modeling Pipeline for the 2014 COCO dataset<br>\n",
        "This notebook loads data from the 2014 COCO training dataset, trains a deep learning model, and evaluates the results. We split the training dataset into training, validation, and testing data since we do not yet require all of the images at this stage in our project development.\n",
        "This notebook uses the LeNet-5 model and utilizes the method outlined in [this article](https://towardsdatascience.com/master-the-coco-dataset-for-semantic-image-segmentation-part-1-of-2-732712631047).",
        " \n",
        "Authors: Péter Hámori, Audrea Huang<br>\n",
        "Date: 11 April 2021<br>\n",
        "AIT Deep Learning<br>\n",
        "Project Milestone 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7J-z6Sq4dvF"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4PKTqwks93w",
        "outputId": "376e9662-1aeb-4c3c-f6d0-542d278c6784"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twc0RtEKtE8B"
      },
      "source": [
        "# visualize outputs\n",
        "%matplotlib inline\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io as io\n",
        "\n",
        "# data processing\n",
        "from pycocotools.coco import COCO\n",
        "import cv2\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "import pylab\n",
        "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIBMs4uFtPiJ"
      },
      "source": [
        "import random\n",
        "\n",
        "# keras\n",
        "from keras.utils import to_categorical \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edBckAfCtE-5"
      },
      "source": [
        "annFileTrain = '/content/drive/MyDrive/Colab Notebooks/AIT_DeepLearning/coco project/instances_train2014.json'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbQuU5BU5ToG"
      },
      "source": [
        "Load annotations into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIwPpesetFEG",
        "outputId": "215c77f6-5cc6-4500-ebc2-d2f158cfe154"
      },
      "source": [
        "# start with small dataset consisting of these classes\n",
        "filterClasses = ['car', 'chair', 'book', 'bottle']\n",
        "\n",
        "# get class IDs for corresponding filterClasses\n",
        "coco=COCO(annFileTrain)\n",
        "catIds = coco.getCatIds(catNms=filterClasses) \n",
        "\n",
        "# get all images containing the category IDs\n",
        "imgIds = coco.getImgIds(catIds=catIds)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=24.71s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWWGzeFm1Ag1"
      },
      "source": [
        "Create helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AdQdxZAtFBf"
      },
      "source": [
        "def getClassName(classID, cats):\n",
        "  '''\n",
        "  Iterate through the categories to extract the desired class name.\n",
        "  :param  classID (int)         : requested class ID\n",
        "  :param  cats (list of strings): requested categories\n",
        "  :return: string: class name or \"None\" if none found\n",
        "  '''\n",
        "  for i in range(len(cats)):\n",
        "      if cats[i]['id']==classID:\n",
        "          return cats[i]['name']\n",
        "  return \"None\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPxU71pytFL2"
      },
      "source": [
        "def getNormalMask(imageObj, classes, coco, catIds, input_image_size):\n",
        "    '''\n",
        "    Mask outlining each specific class of interest.\n",
        "\n",
        "    :param imageObj (dict): input image\n",
        "    :param classes (list of strings): classes of interest\n",
        "    :param coco (COCO): instance annotations\n",
        "    :param catIds (list of integers): category IDs\n",
        "    :param input_image_size (tuple): size of input image\n",
        "    :return: train_mask(ndarray): mask\n",
        "    '''\n",
        "    annIds = coco.getAnnIds(imageObj['id'], catIds=catIds, iscrowd=None)\n",
        "    anns = coco.loadAnns(annIds)\n",
        "    cats = coco.loadCats(catIds)\n",
        "    train_mask = np.zeros(input_image_size)\n",
        "    for a in range(len(anns)):\n",
        "        className = getClassName(anns[a]['category_id'], cats)\n",
        "        pixel_value = classes.index(className)+1\n",
        "        new_mask = cv2.resize(coco.annToMask(anns[a])*pixel_value, input_image_size)\n",
        "        train_mask = np.maximum(new_mask, train_mask)\n",
        "\n",
        "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
        "    train_mask = train_mask.reshape(input_image_size[0], input_image_size[1], 1)\n",
        "    return train_mask  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3OlFkeUtFT8"
      },
      "source": [
        "def getBinaryMask(imageObj, coco, catIds, input_image_size):\n",
        "    '''\n",
        "    Mask indicating which parts of the image correspond to classes of interest.\n",
        "\n",
        "    :param imageObj (dict): input image\n",
        "    :param coco (COCO): instance annotations\n",
        "    :param catIds (list of integers): category IDs\n",
        "    :param input_image_size (tuple): size of input image\n",
        "    :return: train_mask(ndarray): binary mask\n",
        "    '''\n",
        "\n",
        "    annIds = coco.getAnnIds(imageObj['id'], catIds=catIds, iscrowd=None)\n",
        "    anns = coco.loadAnns(annIds)\n",
        "    train_mask = np.zeros(input_image_size)\n",
        "    for a in range(len(anns)):\n",
        "        new_mask = cv2.resize(coco.annToMask(anns[a]), input_image_size)\n",
        "        \n",
        "        #Threshold because resizing may cause extraneous values\n",
        "        new_mask[new_mask >= 0.5] = 1\n",
        "        new_mask[new_mask < 0.5] = 0\n",
        "\n",
        "        train_mask = np.maximum(new_mask, train_mask)\n",
        "\n",
        "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
        "    train_mask = train_mask.reshape(input_image_size[0], input_image_size[1], 1)\n",
        "    return train_mask"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T9Re_3I7dq1"
      },
      "source": [
        "def visualizeImageAndMask(img, mask):\n",
        "    '''\n",
        "    Display image and its corresponding mask.\n",
        "    :param  img(ndarray): specified image\n",
        "    :param  mask(ndarray): corresponding mask  \n",
        "    '''\n",
        " \n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    outerGrid = gridspec.GridSpec(1, 2, wspace=0.1, hspace=0.1)\n",
        "\n",
        "    ax = plt.Subplot(fig, outerGrid[0])\n",
        "    ax.imshow(img);\n",
        "\n",
        "    ax = plt.Subplot(fig, outerGrid[1])\n",
        "    ax.imshow(mask[:,:,0]);\n",
        "\n",
        "    ax.axis('off')\n",
        "    fig.add_subplot(ax)\n",
        "\n",
        "    plt.show()\n",
        "    return"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2M3kLi2tFJL"
      },
      "source": [
        "def getImage(imageObj, input_image_size):\n",
        "    '''\n",
        "    Return a specified image, rescaled to the desired size\n",
        "    :param   imageObj (int)        : requested image object\n",
        "    :param   input_image_size (tuple): size of image\n",
        "    :return: image: image of specified size\n",
        "    '''\n",
        "    # Read and normalize an image\n",
        "    train_img = io.imread(imageObj['coco_url'])/255.0\n",
        "\n",
        "    # Resize\n",
        "    train_img = cv2.resize(train_img, input_image_size)\n",
        "    if (len(train_img.shape)==3 and train_img.shape[2]==3): # If it is a RGB 3 channel image\n",
        "        return train_img\n",
        "    else: # To handle a black and white image, increase dimensions to 3\n",
        "        stacked_img = np.stack((train_img,)*3, axis=-1)\n",
        "        return stacked_img"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZakJPhwwbZnP"
      },
      "source": [
        "def getData(number_of_samples, images, classes, coco, input_image_size=(224,224), \n",
        "            batch_size=4, mode='train', mask_type='binary'):\n",
        "  '''\n",
        "  Get images and corresponding masks.\n",
        "\n",
        "  :param number_of_samples (int): sample size\n",
        "  :param images (list of dictionaries): images in dataset\n",
        "  :param classes (list of strings): object classes of interest\n",
        "  :param coco (COCO): instance annotations\n",
        "  :param input_image_size (tuple): width and height of input\n",
        "  :param batch_size (int): batch size\n",
        "  :param mode (string): train, valid, or test\n",
        "  :param mask_type (string): binary or normal\n",
        "  :return im: list of images\n",
        "  :return m: list of masks\n",
        "  '''\n",
        "  dataset_size = len(images)\n",
        "  catIds = coco.getCatIds(catNms=classes)\n",
        "\n",
        "  im = []\n",
        "  m = []\n",
        "\n",
        "  for i in range(number_of_samples):\n",
        "    imageObj = images[i]\n",
        "\n",
        "    # Retrieve Image\n",
        "    train_img = getImage(imageObj, input_image_size)\n",
        "            \n",
        "    # Create Mask\n",
        "    if mask_type == \"binary\":\n",
        "      train_mask = getBinaryMask(imageObj, coco, catIds, input_image_size)\n",
        "            \n",
        "    elif mask_type == \"normal\":\n",
        "      train_mask = getNormalMask(imageObj, classes, coco, catIds, input_image_size)     \n",
        "\n",
        "    annIds = coco.getAnnIds(imageObj['id'], catIds=catIds, iscrowd=None)\n",
        "\n",
        "    im.append(train_img)\n",
        "    m.append(train_mask)\n",
        "  \n",
        "  return im, m\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fvbv4vxrKQO_"
      },
      "source": [
        "def get_targets(number_of_samples, images, classes, coco, input_image_size=(224,224)):\n",
        "  y = []\n",
        "  \n",
        "  catIds = coco.getCatIds(catNms=classes)\n",
        "  cats = coco.loadCats(catIds)\n",
        "\n",
        "  for i in range(number_of_samples):\n",
        "    o = []\n",
        "    imageObj = images[i]\n",
        "    annIds = coco.getAnnIds(imageObj['id'], catIds=catIds, iscrowd=None)\n",
        "    anns = coco.loadAnns(annIds)\n",
        "    \n",
        "    for a in range(len(anns)):\n",
        "        className = getClassName(anns[a]['category_id'], cats)  \n",
        "        o.append(className)\n",
        "\n",
        "    y.append(o[0])\n",
        "  return y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8kPNmbOtFGo"
      },
      "source": [
        "def filterDataset(annFile, classes=None):  \n",
        "    '''\n",
        "    Extract images corresponding to the specified classes and remove duplicates.\n",
        "\n",
        "    :param  annFile (string): relative path for annotations\n",
        "    :param  classes (list of strings): objects we are interested in detecting\n",
        "    :return: unique_images (list of images): list with only one instance of each \n",
        "      image, which may contain multiple objects of interest\n",
        "    :return: dataset_size (int): size of dataset corresponding to annFile\n",
        "    :return: coco (COCO): COCO object for instance annotations\n",
        "    '''  \n",
        "    # initialize COCO api for instance annotations\n",
        "    coco = COCO(annFile)\n",
        "    \n",
        "    images = []\n",
        "    if classes!=None:\n",
        "        # iterate for each individual class in the list\n",
        "        for className in classes:\n",
        "            # get all images containing given categories\n",
        "            catIds = coco.getCatIds(catNms=className)\n",
        "            imgIds = coco.getImgIds(catIds=catIds)\n",
        "            images += coco.loadImgs(imgIds)\n",
        "    \n",
        "    else:\n",
        "        imgIds = coco.getImgIds()\n",
        "        images = coco.loadImgs(imgIds)\n",
        "    \n",
        "    # Now, filter out the repeated images\n",
        "    unique_images = []\n",
        "    for i in range(len(images)):\n",
        "        if images[i] not in unique_images:\n",
        "            unique_images.append(images[i])\n",
        "            \n",
        "    random.shuffle(unique_images)\n",
        "    dataset_size = len(unique_images)\n",
        "    \n",
        "    return unique_images, dataset_size, coco"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhsiO4ZetFb9",
        "outputId": "93792467-bb2f-4f9d-b06a-f34dbf460ffb"
      },
      "source": [
        "classes = ['car', 'chair', 'book', 'bottle']\n",
        "train_images, train_dataset_size, train_coco = filterDataset(annFileTrain, classes)\n",
        "input_image_size = (224,224)\n",
        "mask_type = 'normal'"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=13.97s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFvtox19QCTV"
      },
      "source": [
        "number_of_samples = 100"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyajdY5LuE_5"
      },
      "source": [
        "t_images = []\n",
        "t_masks = []\n",
        "t_images, t_masks = getData(number_of_samples, train_images, classes, coco, input_image_size, mask_type)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3-oq6cV8KBl"
      },
      "source": [
        "# visualizeImageAndMask(t_images[1], t_masks[1])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE84wDTVbG74"
      },
      "source": [
        "#t_images[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUH6smD4-LS9"
      },
      "source": [
        "def concatenate_image_mask(img, mask):\n",
        "  '''\n",
        "  Combine image and mask to feed to model.\n",
        "  '''\n",
        "  img = img.reshape(224*224*3)\n",
        "  mask = mask.reshape(224*224*1)\n",
        "  concat = np.concatenate((img, mask))\n",
        "  concat = concat.reshape(224,224,4)\n",
        "  return concat"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZhEgTsgAXtp"
      },
      "source": [
        "#CONCATENATING IMAGES AND MASKS\n",
        "X = []\n",
        "for i in range(len(t_images)):\n",
        "  X.append(concatenate_image_mask(t_images[i], t_masks[i]))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WaqFAB_Nb-9"
      },
      "source": [
        "#PREPROCESSING INPUT\n",
        "for i in range(len(X)):\n",
        "    X[i] = X[i]/255.0   #normalizing\n",
        "    X[i] = np.asarray(X[i]) \n",
        "    X[i] = resize(X[i], input_image_size) #reshaping "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9akOIobL9cf"
      },
      "source": [
        "Y = get_targets(number_of_samples, train_images, classes, coco, input_image_size)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI9dn8BaMfxU"
      },
      "source": [
        "#target classes to integers \n",
        "for i in range(len(Y)):\n",
        "    if Y[i] == 'car':\n",
        "      Y[i] = 0\n",
        "    elif Y[i] == 'chair':\n",
        "      Y[i] = 1\n",
        "    elif Y[i] == 'book':\n",
        "      Y[i] = 2\n",
        "    elif Y[i] == 'bottle':\n",
        "      Y[i] = 3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy-xRNeaOE1_"
      },
      "source": [
        "Y = to_categorical(Y, 4)  #one-hot encoding"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhAQWACM3Rmw"
      },
      "source": [
        "Split training data into train, validation, and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2fjsm48Bfd7"
      },
      "source": [
        "# TRAIN-VALIDATION-TEST SETS\n",
        "t_point = int(0.7*len(X))\n",
        "v_point = int(0.8*len(X))\n",
        "\n",
        "X_train = []\n",
        "X_val = []\n",
        "X_test = []\n",
        "X_train = X[:t_point]\n",
        "X_val = X[t_point:v_point]\n",
        "X_test = X[v_point:]\n",
        "\n",
        "Y_train = []\n",
        "Y_val = []\n",
        "Y_test = []\n",
        "Y_train = Y[:t_point]\n",
        "Y_val = Y[t_point:v_point]\n",
        "Y_test = Y[v_point:]\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWXbeHVh3WH5"
      },
      "source": [
        "### Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW2jJqXv6E1r"
      },
      "source": [
        "from keras.layers import Conv2D , AveragePooling2D , Dense, Flatten\n",
        "from keras.models import Sequential"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcFdsY7-wMDO"
      },
      "source": [
        "model = Sequential(name=\"LeNet-5\")\n",
        "model.add(Conv2D(6,(5,5),strides=(1,1), activation='tanh',input_shape=(input_image_size[0], input_image_size[1], 3+1))) \n",
        "model.add(AveragePooling2D())\n",
        "model.add(Conv2D(16,(5,5),strides=(1,1),activation='tanh'))\n",
        "model.add(AveragePooling2D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120,activation='tanh')) \n",
        "model.add(Dense(84,activation='tanh')) \n",
        "model.add(Dense(4,activation='softmax')) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7urK_ziOThcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1fdf4f6-ef8a-4559-a5f5-e3334f350f92"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"LeNet-5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 220, 220, 6)       606       \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 110, 110, 6)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 106, 106, 16)      2416      \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 53, 53, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 44944)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 120)               5393400   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 84)                10164     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 340       \n",
            "=================================================================\n",
            "Total params: 5,406,926\n",
            "Trainable params: 5,406,926\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw9dYmxLaND_"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu1ADYKuaNIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825e9961-75f0-4447-9574-ad5817af45c2"
      },
      "source": [
        "model.fit(np.asarray(X_train), Y_train, epochs=30, batch_size=32, validation_data=(np.asarray(X_val), Y_val))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "3/3 [==============================] - 4s 1s/step - loss: 1.3965 - accuracy: 0.1372 - val_loss: 1.1742 - val_accuracy: 0.6000\n",
            "Epoch 2/30\n",
            "3/3 [==============================] - 3s 1000ms/step - loss: 1.3746 - accuracy: 0.3212 - val_loss: 1.3610 - val_accuracy: 0.2000\n",
            "Epoch 3/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.3527 - accuracy: 0.2984 - val_loss: 1.2711 - val_accuracy: 0.2000\n",
            "Epoch 4/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.3810 - accuracy: 0.2529 - val_loss: 1.2875 - val_accuracy: 0.2000\n",
            "Epoch 5/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.3401 - accuracy: 0.3654 - val_loss: 1.2188 - val_accuracy: 0.7000\n",
            "Epoch 6/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.3086 - accuracy: 0.3127 - val_loss: 1.1559 - val_accuracy: 0.6000\n",
            "Epoch 7/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.3215 - accuracy: 0.3003 - val_loss: 1.1100 - val_accuracy: 0.6000\n",
            "Epoch 8/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.2869 - accuracy: 0.3557 - val_loss: 1.1581 - val_accuracy: 0.6000\n",
            "Epoch 9/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.2561 - accuracy: 0.4278 - val_loss: 1.4383 - val_accuracy: 0.1000\n",
            "Epoch 10/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.2139 - accuracy: 0.5117 - val_loss: 1.2391 - val_accuracy: 0.5000\n",
            "Epoch 11/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.2548 - accuracy: 0.5326 - val_loss: 1.2208 - val_accuracy: 0.6000\n",
            "Epoch 12/30\n",
            "3/3 [==============================] - 3s 994ms/step - loss: 1.1264 - accuracy: 0.4850 - val_loss: 1.5094 - val_accuracy: 0.3000\n",
            "Epoch 13/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.1330 - accuracy: 0.4915 - val_loss: 1.4444 - val_accuracy: 0.1000\n",
            "Epoch 14/30\n",
            "3/3 [==============================] - 3s 995ms/step - loss: 1.1010 - accuracy: 0.4890 - val_loss: 1.4712 - val_accuracy: 0.2000\n",
            "Epoch 15/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 1.0797 - accuracy: 0.5533 - val_loss: 1.5243 - val_accuracy: 0.4000\n",
            "Epoch 16/30\n",
            "3/3 [==============================] - 3s 982ms/step - loss: 1.0118 - accuracy: 0.5384 - val_loss: 1.3318 - val_accuracy: 0.4000\n",
            "Epoch 17/30\n",
            "3/3 [==============================] - 3s 994ms/step - loss: 0.9574 - accuracy: 0.5292 - val_loss: 1.4511 - val_accuracy: 0.3000\n",
            "Epoch 18/30\n",
            "3/3 [==============================] - 3s 986ms/step - loss: 0.8378 - accuracy: 0.7087 - val_loss: 1.4496 - val_accuracy: 0.2000\n",
            "Epoch 19/30\n",
            "3/3 [==============================] - 3s 990ms/step - loss: 0.8182 - accuracy: 0.6749 - val_loss: 1.5029 - val_accuracy: 0.4000\n",
            "Epoch 20/30\n",
            "3/3 [==============================] - 3s 1000ms/step - loss: 0.7648 - accuracy: 0.7002 - val_loss: 1.8265 - val_accuracy: 0.4000\n",
            "Epoch 21/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.6090 - accuracy: 0.7653 - val_loss: 1.8305 - val_accuracy: 0.4000\n",
            "Epoch 22/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.5745 - accuracy: 0.8765 - val_loss: 1.8066 - val_accuracy: 0.3000\n",
            "Epoch 23/30\n",
            "3/3 [==============================] - 3s 993ms/step - loss: 0.5097 - accuracy: 0.8355 - val_loss: 1.8050 - val_accuracy: 0.3000\n",
            "Epoch 24/30\n",
            "3/3 [==============================] - 3s 1s/step - loss: 0.4987 - accuracy: 0.8661 - val_loss: 2.2360 - val_accuracy: 0.3000\n",
            "Epoch 25/30\n",
            "3/3 [==============================] - 3s 999ms/step - loss: 0.4351 - accuracy: 0.8622 - val_loss: 2.3232 - val_accuracy: 0.2000\n",
            "Epoch 26/30\n",
            "3/3 [==============================] - 3s 997ms/step - loss: 0.3369 - accuracy: 0.9213 - val_loss: 1.5438 - val_accuracy: 0.6000\n",
            "Epoch 27/30\n",
            "3/3 [==============================] - 3s 992ms/step - loss: 0.4051 - accuracy: 0.8583 - val_loss: 1.5778 - val_accuracy: 0.5000\n",
            "Epoch 28/30\n",
            "3/3 [==============================] - 3s 978ms/step - loss: 0.2754 - accuracy: 0.9252 - val_loss: 2.0253 - val_accuracy: 0.4000\n",
            "Epoch 29/30\n",
            "3/3 [==============================] - 3s 981ms/step - loss: 0.2795 - accuracy: 0.9103 - val_loss: 2.4917 - val_accuracy: 0.3000\n",
            "Epoch 30/30\n",
            "3/3 [==============================] - 3s 997ms/step - loss: 0.2610 - accuracy: 0.9434 - val_loss: 2.5635 - val_accuracy: 0.2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc62ad6b350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bytcjsni3cbt"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjhrAjd2aNKL",
        "outputId": "c3f8d6b6-ac3b-41e5-da67-9a131dc5bf06"
      },
      "source": [
        "acc = model.evaluate(np.asarray(X_test), Y_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 390ms/step - loss: 2.2192 - accuracy: 0.3000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
